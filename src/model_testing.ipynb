{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitmlpconda7f86b9b3c7f847e99c33aa47d858c0d9",
   "display_name": "Python 3.8.5 64-bit ('mlp': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import defaultdict\n",
    "from nltk.metrics import precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# We only need the sentence splitting and word embedding features, disabling the\n",
    "# unneed features makes this a lot faster.\n",
    "nlp = spacy.load('en_core_web_md', disable=['tagger', 'ner'])\n",
    "\n",
    "'''\n",
    "TODO: word embeddings aan de dataset toevoegen\n",
    "Denk dat bij het inlezen dit het beste is om te doen, dan hoeven we maar 1x door alles\n",
    "heen te loopen. Even uitzoeken hoe de embedding vectors meegegeven moeten worden \n",
    "(dict net als BOW of tuple met (<lijst met vectors>, <label>))\n",
    "\n",
    "Even experimenteren met wat beter werkt: \n",
    "    - soort bow van individuele token vectors per review\n",
    "    - of een gecombineerde methode gebruiken \n",
    "        -> gemiddelde van alle token vectors\n",
    "        -> doc vector\n",
    "        -> ???\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/wessel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "  Training set: 34124\n",
      "  Test set: 4266\n",
      "  Development set: 4266\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import sys\n",
    "from string import punctuation\n",
    "import pickle\n",
    "\n",
    "from nltk import download, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the nltk stopwords corpus if needed\n",
    "download('stopwords')\n",
    "\n",
    "\n",
    "def create_dataset(filepath):\n",
    "    ''' Reads and cleans the csv file and structures the datapoints ''' \n",
    "    dataset = []\n",
    "\n",
    "    # The translation and set are both a lot faster O(1) \n",
    "    # when compared to checking a list or string O(n).\n",
    "    punct_translation = str.maketrans('', '', punctuation)\n",
    "    stoplist = set(stopwords.words('english'))\n",
    "\n",
    "    with open(filepath, 'r', encoding='latin-1') as f:\n",
    "        reader = csv.reader(f, delimiter=\",\", )\n",
    "        \n",
    "        # Skip the header row\n",
    "        next(reader, None)\n",
    "\n",
    "        # Items per row:\n",
    "        #   0 -> review id\n",
    "        #   1 -> rating between 1-5\n",
    "        #   2 -> year and month\n",
    "        #   3 -> location of reviewer\n",
    "        #   4 -> review text\n",
    "        #   5 -> Disneyland location\n",
    "        for row in reader:\n",
    "            rating = int(row[1])\n",
    "\n",
    "            if rating < 3:\n",
    "                rating_label = 'negative'\n",
    "            elif rating == 3:\n",
    "                rating_label = 'indifferent'\n",
    "            else:\n",
    "                rating_label = 'positive'\n",
    "\n",
    "            review_text = row[4] \\\n",
    "                .translate(punct_translation) \\\n",
    "                .lower() \\\n",
    "                .strip()\n",
    "\n",
    "            tokenized = [\n",
    "                token for token in word_tokenize(review_text)\n",
    "                if token not in stoplist\n",
    "            ]\n",
    "\n",
    "            bag_of_words = ({t: True for t in tokenized}, rating_label)\n",
    "\n",
    "            dataset.append(\n",
    "                {'tokenized': tokenized, \n",
    "                 'bag_of_words': bag_of_words, \n",
    "                 'rating_label': rating_label, \n",
    "                 'year_month': row[2], \n",
    "                 'reviewer_location': row[3], \n",
    "                 'review_text': row[4], \n",
    "                 'disneyland_location': row[5],\n",
    "                 'doc_vector': nlp(' '.join(tokenized)).vector\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_train_test(feats, split=0.8):\n",
    "    ''' Creates test, train and dev splits from the dataset ''' \n",
    "    random.Random(1).shuffle(feats)\n",
    "\n",
    "    cutoff = int(len(feats) * split)\n",
    "    tenpercent = int((len(feats) - cutoff) / 2)\n",
    "    split = cutoff + tenpercent\n",
    "\n",
    "    train_feats = feats[:cutoff]\n",
    "    test_feats = feats[cutoff:split]\n",
    "    dev_feats = feats[split:]\n",
    "\n",
    "    print(\"  Training set: %i\" % len(train_feats))\n",
    "    print(\"  Test set: %i\" % len(test_feats))\n",
    "    print(\"  Development set: %i\" % len(dev_feats))\n",
    "\n",
    "    return train_feats, test_feats, dev_feats\n",
    "dataset = create_dataset('../data/DisneylandReviews.csv')\n",
    "\n",
    "train_feats, test_feats, dev_feats = split_train_test(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(classifier, testfeats):\n",
    "\trefsets = defaultdict(set)\n",
    "\ttestsets = defaultdict(set)\n",
    "\t\n",
    "\tfor i, (feats, label) in enumerate(testfeats):\n",
    "\t\trefsets[label].add(i)\n",
    "\t\tobserved = classifier.classify(feats)\n",
    "\t\ttestsets[observed].add(i)\n",
    "\t\n",
    "\tprecisions = {}\n",
    "\trecalls = {}\n",
    "\t\n",
    "\tfor label in classifier.labels():\n",
    "\t\tprecisions[label] = precision(refsets[label], testsets[label])\n",
    "\t\trecalls[label] = recall(refsets[label], testsets[label])\n",
    "\t\n",
    "\treturn precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f(precisions, recalls):\n",
    "    f_measures = {}\n",
    "\n",
    "    for category in precisions.keys():\n",
    "        # This is done to prevent the program from crashing when \n",
    "        # no measure is provided for a particular category\n",
    "        if not precisions[category] or not recalls[category]:\n",
    "            f_measures[category] = None\n",
    "            continue\n",
    "\n",
    "        f_measures[category] = round(\n",
    "            2 * ((precisions[category] * recalls[category]) /\n",
    "                 (precisions[category] + recalls[category])), 6)\n",
    "\n",
    "    return f_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(classifier, test_feats, categories):\n",
    "    \"\"\" Taken from assignment 1, calculates and prints evaluation measures \"\"\"\n",
    "    print(\"\\n##### Evaluation...\")\n",
    "    print(\"  Accuracy: %f\" % nltk.classify.accuracy(classifier, test_feats))\n",
    "    precisions, recalls = precision_recall(classifier, test_feats)\n",
    "    f_measures = calculate_f(precisions, recalls)\n",
    "\n",
    "    print(\" |-----------|-----------|-----------|-----------|\")\n",
    "    print(\" |%-11s|%-11s|%-11s|%-11s|\" %\n",
    "          (\"category\", \"precision\", \"recall\", \"F-measure\"))\n",
    "    print(\" |-----------|-----------|-----------|-----------|\")\n",
    "    for category in categories:\n",
    "        if precisions[category] is None:\n",
    "            print(\" |%-11s|%-11s|%-11s|%-11s|\" % (category, \"NA\", \"NA\", \"NA\"))\n",
    "        else:\n",
    "            print(\" |%-11s|%-11f|%-11f|%-11s|\" %\n",
    "                  (category,\n",
    "                   precisions[category],\n",
    "                   recalls[category],\n",
    "                   f_measures[category])\n",
    "                  )\n",
    "    print(\" |-----------|-----------|-----------|-----------|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train_feats):\n",
    "    ''' Trains and returns a linear SVM classifier '''\n",
    "    return SklearnClassifier(LinearSVC(dual=False)).train(train_feats)\n",
    "\n",
    "def train_knn(train_feats):\n",
    "    ''' Trains and returns a KNN classifier '''\n",
    "    return SklearnClassifier(KNeighborsClassifier()).train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_bow_test = [item['bag_of_words'] for item in test_feats]\n",
    "only_bow_train = [item['bag_of_words'] for item in train_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = train_svm(only_bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = train_knn(only_bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "##### Evaluation...\n",
      "  Accuracy: 0.812705\n",
      " |-----------|-----------|-----------|-----------|\n",
      " |category   |precision  |recall     |F-measure  |\n",
      " |-----------|-----------|-----------|-----------|\n",
      " |positive   |0.889932   |0.932638   |0.910784   |\n",
      " |indifferent|0.364425   |0.321224   |0.341463   |\n",
      " |negative   |0.588235   |0.438144   |0.502216   |\n",
      " |-----------|-----------|-----------|-----------|\n",
      "\n",
      "##### Evaluation...\n",
      "  Accuracy: 0.786451\n",
      " |-----------|-----------|-----------|-----------|\n",
      " |category   |precision  |recall     |F-measure  |\n",
      " |-----------|-----------|-----------|-----------|\n",
      " |positive   |0.791835   |0.994337   |0.881607   |\n",
      " |indifferent|0.263158   |0.019120   |0.035651   |\n",
      " |negative   |0.600000   |0.023196   |0.044665   |\n",
      " |-----------|-----------|-----------|-----------|\n"
     ]
    }
   ],
   "source": [
    "evaluation(svm_classifier, only_bow_test, ['positive', 'indifferent', 'negative'])\n",
    "evaluation(knn_classifier, only_bow_test, ['positive', 'indifferent', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_we_test = [(item['doc_vector'], item['rating_label']) for item in test_feats]\n",
    "only_we_train = [(item['doc_vector'], item['rating_label']) for item in train_feats]\n",
    "\n",
    "only_vec_train = [i[0] for i in only_we_train]\n",
    "only_label_train = [j[1] for j in only_we_train]\n",
    "\n",
    "only_vec_test = [i[0] for i in only_we_test]\n",
    "only_label_test = [j[1] for j in only_we_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = LinearSVC().fit(only_vec_train, only_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8211439287388654"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "svm_classifier.score(only_vec_test, only_label_test)"
   ]
  },
  {
   "source": [
    "# Acc met onbewerkte, ruwe review text als doc vector (spacy schoont en splitst)\n",
    "- SVM: 0.8218471636193155\n",
    "- KNN: 0.7740271917487107\n",
    "\n",
    "# Acc met tokenized en geschoonde tokens als doc vector \n",
    "- SVM: 0.8211439287388654\n",
    "- KNN: 0.7805907172995781\n",
    "\n",
    "# TODO\n",
    "- Uitproberen met individuele token vectors en niet de 'platgeslagen' doc vector.\n",
    "- Andere items uit de dataset als features toevoegen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7805907172995781"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "knn_classifier = KNeighborsClassifier().fit(only_vec_train, only_label_train)\n",
    "knn_classifier.score(only_vec_test, only_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))"
   ]
  }
 ]
}